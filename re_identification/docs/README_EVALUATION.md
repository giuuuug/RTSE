# Evaluation of Re-Identification model

Our evaluation service is a comprehensive tool that enables users to assess the performances of their ONNX (float or QDQ), TensorFlow Lite (.tflite), or Keras (.keras) re-identification models. By uploading their model and a test set (query and gallery), users can quickly and easily evaluate the performance of their model and generate various metrics, such as Rank-N accuracy, mean average precision (mAP), and CMC curve.

The evaluation service is designed to be fast, efficient, and accurate, making it an essential tool for anyone looking to evaluate the performance of their re-identification model.

Unlike the other use cases, the re-identification scenario requires **two separate evaluation datasets** (distinct from the training and validation sets): a **query set** and a **gallery set**.  

- The **query set** contains images of persons to be identified.  
- The **gallery set** contains images of persons with known identities.  

The model extracts feature embeddings from both sets, and a distance metric is then applied to compare these features in order to find matches between query and gallery images.  

We currently support two distance metrics for this evaluation: **cosine** and **Euclidean**.

<details open>
<summary><a href="#1"><b>1. Configure the YAML file</b></a></summary><a id="1"></a>

To use this service and achieve your goals, you can use the [user_config.yaml](../user_config.yaml) or directly update the [evaluation_config.yaml](../config_file_examples/evaluation_config.yaml) file and use it. This file provides an example of how to configure the evaluation service to meet your specific needs.

Alternatively, you can follow the tutorial below, which shows how to evaluate your pre-trained re-identification model using our evaluation service.

<ul>
<details open><summary><a href="#1-1">1.1 Set the model and the operation mode</a></summary><a id="1-1"></a>

As mentioned previously, all the sections of the YAML file must be set in accordance with this **[evaluation_config.yaml](../config_file_examples/evaluation_config.yaml)**.
In particular, `operation_mode` should be set to evaluation and the `evaluation` section should be filled as in the following example: 

```yaml
model:
   model_path: ../../stm32ai-modelzoo/re_identification/osnet/ST_pretrainedmodel_public_dataset/DeepSportradar/osnet_a100_256_128_tfs/osnet_a100_256_128_tfs_int8.tflite

operation_mode: evaluation
```
In this example, the path to your re-identification model is provided in the `model_path` parameter.

```yaml
evaluation:
   target: host # host, stedgeai_host, stedgeai_n6
   reid_distance_metric: cosine # Options: cosine, euclidean
```
In the 'evaluation' section, if users are using a quantized TFLITE or ONNX model, they can decide to do the inferences with the classic python interpreters (host -> by default), with the C code generated by stedgeai on the PC (stedgeai_host), or with the C code generated by stedgeai on the N6 board directly (stedgeai_n6) using the `target` attribute.

The `reid_distance_metric` attribute allows users to choose the distance metric used for re-identification evaluation. It can be set to either "cosine" or "euclidean", depending on the user's preference.

</details>

<details open><summary><a href="#1-2">1.2 Prepare the dataset</a></summary><a id="1-2"></a>

Information about the dataset you want to use for evaluation is provided in the `dataset` section of the configuration file, as shown in the YAML code below.

```yaml
dataset:
  dataset_name: DeepSportradar
  test_query_path: ./datasets/DeepSportradar-ReID/reid_test/query
  test_gallery_path: ./datasets/DeepSportradar-ReID/reid_test/gallery
```
In this example, the path to the test set is provided in the `test_query_path` and `test_gallery_path` parameters. Here the two paths are necessary to evaluate the re-identification model.

When working with a dataset for the first time, we suggest setting the `check_image_files` attribute to True. This will enable the system to load each image file and identify any corrupt, unsupported, or non-image files. The path to any problematic files will be reported.



</details>

<details open><summary><a href="#1-3">1.3 Apply preprocessing</a></summary><a id="1-3"></a>

The images from the dataset need to be preprocessed before they are presented to the network for evaluation.
This includes rescaling and resizing. In particular, they need to be rescaled exactly as they were at the training step.
This is illustrated in the YAML code below:

```yaml
preprocessing:
   rescaling: { scale: 1/127.5, offset: -1 }
   resizing: 
     aspect_ratio: "fit"
     interpolation: nearest
   color_mode: rgb
```

In this example, the pixels of the input images read in the dataset are in the interval [0, 255], that is UINT8. If you set `scale` to 1./255 and `offset` to 0, they will be rescaled to the interval [0.0, 1.0]. 
If you set `scale` to 1/127.5 and `offset` to -1, they will be rescaled to the interval [-1.0, 1.0].

The `resizing` attribute specifies the image resizing methods you want to use:
- The value of `interpolation` must be one of *{"bilinear", "nearest", "bicubic", "area", "lanczos3", "lanczos5", "gaussian", "mitchellcubic"}*.
- The value of `aspect_ratio` must be either *"fit"* or *"crop"*. If you set it to *"fit"*, the resized images will be distorted if their original aspect ratio is not the same as in the resizing size. 
If you set it to *"crop"*, images will be cropped as necessary to preserve the aspect ratio.

The `color_mode` attribute must be one of "*grayscale*", "*rgb*" or "*rgba*".

</details>

<details open><summary><a href="#1-4">1.4 Hydra and MLflow settings</a></summary><a id="1-4"></a>

The `mlflow` and `hydra` sections must always be present in the YAML configuration file. The `hydra` section can be used to specify the name of the directory where experiment directories are saved and/or the pattern used to name experiment directories. With the YAML code below, every time you run the Model Zoo, an experiment directory is created that contains all the directories and files created during the run. The names of experiment directories are all unique as they are based on the date and time of the run.

```yaml
hydra:
   run:
      dir: ./tf/src/experiments_outputs/${now:%Y_%m_%d_%H_%M_%S}
```

The `mlflow` section is used to specify the location and name of the directory where MLflow files are saved, as shown below:

```yaml
mlflow:
   uri: ./tf/src/experiments_outputs/mlruns
```

</details>
</ul>
</details>

<details open>
<summary><a href="#2"><b>2. Evaluate your model</b></a></summary><a id="2"></a>

If you chose to modify the [user_config.yaml](../user_config.yaml), you can evaluate the model by running the following code from the UC folder:

```bash
python stm32ai_main.py 
```
If you chose to update the [evaluation_config.yaml](../config_file_examples/evaluation_config.yaml) and use it, then run the following code from the UC folder:

```bash
python stm32ai_main.py --config-path ./config_file_examples/ --config-name evaluation_config.yaml
```
In case you want to evaluate the accuracy of the quantized model then benchmark it, you can either launch the evaluation operation mode followed by the [benchmark service](./README_BENCHMARKING.md) that describes in detail how to proceed or you can use chained services like launching **[chain_eqeb](../config_file_examples/chain_eqeb_config.yaml)** example with the code below:

```bash
python stm32ai_main.py --config-path ./config_file_examples/ --config-name chain_eqeb_config.yaml
```

</details>

<details open>
<summary><a href="#3"><b>3. Visualize the evaluation results</b></a></summary><a id="3"></a>

You can retrieve the CMC curve and mean average precision (mAP) generated after evaluating the float/quantized re-identification model on the test set by navigating to the appropriate directory within **experiments_outputs/<date-and-time>**.

You can also find the evaluation results saved in the log file **stm32ai_main.log** under **experiments_outputs/<date-and-time>**.

</details>
