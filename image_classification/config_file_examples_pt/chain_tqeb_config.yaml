general:
  project_name: '' 
  output: '' 
  saved_models_dir: saved_models
  display_figures: False 
  seed: 42 
  gpu_memory_limit: 3 
  workers: 4 
  log_interval: 50
  recovery_interval: 0
  checkpoint_hist: 10
  save_images: False
  amp: false
  amp_dtype: "float16"
  amp_impl: "native"
  no_ddp_bb: false
  synchronize_step: false
  pin_mem: false 
  no_prefetcher: true
  eval_metric: "top1"
  tta: 0
  local_rank: 0
  use_multi_epochs_loader: false
  log_wandb: false
  log_tb: false

operation_mode: chain_tqeb 

dataset:
  class_names: ''
  classes_file_path: ./datasets/deployment_labels_imagenet.txt
  dataset_name: "imagenet" # options "flowers102", "food101", "imagenet"
  num_classes: 1000 # change according to your dataset
  data_dir: "/local/datasets/" # there shud be imagenet folder inside this directory # can also be used for quantization
  # Two OPTIONAL variables for imagenet data, signifies sub foldar names.
  train_split: "train" # default is 'train'
  val_split: "val" # default is 'val'
  quantization_split: 0.01
  # test_path: you can provide path to test data and it will be used for validation
  # quantization_path: this will be used for quantization otherwise training data will be used

preprocessing: 
  rescaling:
    scale: 1/255.0 # TODO scale node is already present under data_augmentation
    offset: 0
  resizing:
    interpolation: nearest # nearest 'Image resize interpolation type (overrides model)'
    aspect_ratio: fit
  color_mode: rgb
  mean: [0.485, 0.456, 0.406] # 'Override mean pixel value of dataset'
  std: [0.229, 0.224, 0.225] # 'Override std deviation of dataset'

data_augmentation:
  no_aug: False  
  scale: [0.08, 1.0] # TODO scale node is already present under data_augmentation
  ratio: [0.75, 1.33]
  horizontal_flip: 0.5
  vertical_flip: 0.0
  hflip: 0.5
  vflip: 0.0
  color_jitter: 0.4
  aa: null 
  aug_repeats: 0
  aug_splits: 0
  jsd_loss: False
  bce_loss: False
  bce_target_thresh: null
  reprob: 0 
  remode: 'pixel' 
  recount: 1 
  resplit: False  
  mixup: 0.0
  cutmix: 0.0
  cutmix_minmax: null  # Example: [0.3, 0.8]
  mixup_prob: 1.0
  mixup_switch_prob: 0.5
  mixup_mode: "batch"
  smoothing: 0.1
  train_interpolation: "random"
  drop: 0.0
  drop_connect: null
  drop_path: null
  drop_block: null

model: 
  framework: 'torch'
  model_name: 'mobilenetb_a025_pt' 
  pretrained: False
  input_shape: [3, 224, 224]

training:
  epochs: 2
  batch_size: 128
  validation_batch_size: null
  optimizer:
    opt: 'sgd' 
    opt-eps: null 
    opt-betas: null 
    momentum: 0.9
    weight_decay: !!float 2e-5
    clip_grad: null
    clip_mode: 'norm'
    layer_decay: null
  lr_scheduler:
    sched: 'cosine'
    sched_on_updates: False
    lr: null
    lr_base: 0.1
    lr_base_size: 256
    lr_base_scale: ''
    lr_noise: null
    lr_noise_pct: 0.67
    lr_noise_std: 1.0
    lr_cycle_mul: 1.0
    lr_cycle_decay: 0.5
    lr_cycle_limit: 1
    lr_k_decay: 1.0
    warmup_lr: !!float 1e-5
    min_lr: 0
    epoch_repeats: 0
    start_epoch: 0
    decay_milestones: [90, 180, 270]
    decay_epochs: 90 
    warmup_epochs: 5
    warmup_prefix: False
    cooldown_epochs: 0
    patience_epochs: 10
    decay_rate: 0.1
  bn_momentum: null
  bn_eps: null
  sync_bn: false
  dist_bn: "reduce"
  split_bn: false
  model_ema: false
  model_ema_force_cpu: false
  model_ema_decay: 0.9998
  worker_seeding: all

quantization:
   quantizer: Onnx_quantizer
   quantization_type: PTQ
   quantization_input_type: uint8
   quantization_output_type: float
   export_dir: quantized_models

tools:
   stedgeai:
      optimization: balanced
      on_cloud: True
      path_to_stedgeai: C:/ST/STEdgeAI/<x.y>/Utilities/windows/stedgeai.exe
   path_to_cubeIDE: C:/ST/STM32CubeIDE_<*.*.*>/STM32CubeIDE/stm32cubeide.exe
   
benchmarking:
   board: STM32N6570-DK #STM32H747I-DISCO

mlflow:
  uri: ./pt/src/experiments_outputs/mlruns

hydra:
  run:
    dir: ./pt/src/experiments_outputs/${now:%Y_%m_%d_%H_%M_%S}