###################################################################################
#   Copyright (c) 2024 STMicroelectronics.
#   All rights reserved.
#   This software is licensed under terms that can be found in the LICENSE file in
#   the root directory of this software component.
#   If no LICENSE file comes with this software, it is provided AS-IS.
###################################################################################
"""
Utility functions to read the json file generated by the NPU/ATONN/NeuralART compiler
"""

import logging
import os
import json
from typing import Union, Any, Optional
from pathlib import Path
from collections import Counter

try:
    from .logging_utilities import print_table, get_print_fcts
    from .exceptions import ParserJsonError, ErrorException
except ImportError:
    from logging_utilities import print_table, get_print_fcts
    from exceptions import ParserJsonError, ErrorException

#
# History
#   v0.1 - initial version - minimal reader
#   v0.2 - align with the new json format (NODE_EC -> sub-epochs NODE_HW)
#

__title__ = 'NPU Utility - Generated network json reader'
__version__ = '0.2'
__author__ = 'STMicroelectronics'


def _compute_bw_mbs(n_rw: int, n_cycles: int, freq: int = 1000000000) -> float:
    """Compute BW (MB/s)"""
    bw_ = n_rw * freq / n_cycles if n_cycles else 0.0
    return bw_ / (1024 * 1024)


class CNpuNetworkJson():
    """Class to read the generated JSON file"""

    def __init__(self, filepath: Union[Path, str],
                 logger: Union[str, logging.Logger, None] = None,
                 f_name: str = 'network'):
        """Constructor"""

        if isinstance(logger, str) or logger is None:
            logger = logging.getLogger()

        self._logger = logger
        self._filepath = Path(filepath)
        cdts_ = [f'{f_name}_c_info.json', f'{f_name}.json', 'c_info.json']
        if os.path.isdir(self._filepath):
            for cdt_ in cdts_:
                json_fpath = self._filepath / cdt_
                if os.path.isfile(json_fpath):
                    self._filepath = json_fpath

        if not os.path.isfile(self._filepath):
            msg_ = f'CNpuNetworkJson: \'{self._filepath}\' is not valid, cdts={cdts_}'
            raise ParserJsonError(msg_)

        logger.debug('')
        logger.debug('-> CNpuNetworkJson (v%s), file=\"%s\"', __version__, self._filepath)

        with open(self._filepath, encoding="utf-8") as file_handle:
            self._data = json.load(file_handle)

        # checking version
        ver_ = self._data.get("json_schema_version", None)
        if ver_ is None or ver_ != '2.0':
            msg_ = f'CNpuNetworkJson: Json schema is not valid: {ver_} instead 2.0 - {self._filepath}'
            raise ParserJsonError(msg_)

        logger.debug('JSON schema version : %s', ver_)

        graphs_ = self._data.get('graphs', None)
        if graphs_ is None:
            msg_ = f'CNpuNetworkJson: Json file is not valid (should be re-generated): {self._filepath}'
            raise ParserJsonError(msg_)

        # create flatten list of the epochs/nodes with the id as key (_EC -> _HWs)
        #  name is also updated to be "more" readable
        nodes_ = {node['id']: node for node in graphs_[0]['nodes']}
        s_graphs_ = [node for node in nodes_.values() if node['mapping'] == 'NODE_EC']
        for node_ in nodes_.values():
            eid_: int = int(node_['name'].split('_')[-1])
            last_eid: int = -1
            if node_['mapping'] == 'NODE_EC':
                for s_node_ in node_['subgraph_nodes']:
                    last_eid = int(s_node_['name'].split('_')[-1])
            n_name_ = f'EpochBlock_{eid_}'
            if last_eid >= 0:
                n_name_ += f' -> {last_eid}'
            node_['name'] = n_name_
            node_['epoch_id'] = eid_
            node_['parent_id'] = None
            if node_['mapping'] == 'NODE_EC':
                for s_node_ in node_['subgraph_nodes']:
                    s_eid_ = int(s_node_['name'].split('_')[-1])
                    n_name_ = f'EpochBlock_{s_eid_}'
                    s_node_['name'] = n_name_
                    s_node_['parent_id'] = s_node_['id']
                    s_node_['epoch_id'] = eid_
                    if node_['parent_id']:
                        node_['parent_id'].append(s_node_['id'])
                    else:
                        node_['parent_id'] = [s_node_['id']]
        for s_graphs in s_graphs_:
            for s_node_ in s_graphs['subgraph_nodes']:
                nodes_[s_node_['id']] = s_node_

        # ordered by node id
        nodes_ = dict(sorted(nodes_.items()))

        # add the operation type by node
        for node_ in nodes_.values():
            fcts_ = []
            if node_['mapping'] not in ['NODE_NO_X', 'NODE_EC']:
                for s_node_ in node_['subgraph_nodes']:
                    fcts_.append(s_node_['type'])
            node_['functions'] = dict(Counter(fcts_))

        self._epochs = {}
        self._n_ops: int = 0
        self._n_compute_cycles: int = 0
        self._n_max_cycles: int = 0

        for node_ in nodes_.values():
            node_id_ = node_['id']
            name_ = node_["name"]
            o_map_ = node_["mapping"]
            if node_['parent_id'] is not None:
                if isinstance(node_['parent_id'], list):
                    map_ = f'{o_map_} /{len(node_["parent_id"])}'
                else:
                    map_ = f'{o_map_} /{nodes_[node_["parent_id"]]["epoch_id"]}'
            else:
                map_ = o_map_
            item = {
                'name': name_,
                'mapping': map_,
                'ops': 0,
                'compute_cycles': 0,
                'max_cycles': 0,
                'mem_accesses': {},
                'functions': node_['functions']
            }
            self._epochs[node_id_] = item

        # retreive the ops/compute cycles and max cycles by epoch
        pwr_estimates = self._data['power_estimates']
        for pwr_ in pwr_estimates:
            node_id_ = pwr_["node_id"]
            # name_ = nodes_[node_id_]["name"]
            self._n_ops += int(pwr_["ops"])
            self._n_compute_cycles += int(pwr_["compute_cycles"])
            self._n_max_cycles += int(pwr_["max_cycles"])
            item = {
                'ops': int(pwr_["ops"]),
                'compute_cycles': int(pwr_["compute_cycles"]),
                'max_cycles': int(pwr_["max_cycles"]),
                'mem_accesses': {}
            }
            self._epochs[node_id_].update(item)

        # retreive the memory accesses by epoch and by mempool
        mpools_ = {mpool['id']: mpool for mpool in self._data['memory_pools']}
        mem_accesses_ = self._data['memory_accesses']

        for mem_acc in mem_accesses_:
            if mem_acc["mpool_id"] not in mpools_:
                continue
            node_id_ = mem_acc["node_id"]
            mpool_name_ = mpools_[mem_acc["mpool_id"]]["name"]
            m_cycles_ = self._epochs[node_id_]['max_cycles']
            # m_cycles_ = mem_acc['read_cycles'] + mem_acc['write_cycles']
            n_rw_ = mem_acc['reads'] + mem_acc['writes']
            bw_ = _compute_bw_mbs(n_rw_, m_cycles_)
            val_ = (mem_acc['reads'], mem_acc['read_cycles'],
                    mem_acc['writes'], mem_acc['write_cycles'], bw_)
            self._epochs[node_id_]['mem_accesses'][mpool_name_] = val_

        # log a short description
        self.summary(logger.debug)
        logger.debug('')

        msg_ = f'<- {str(self)}'
        logger.debug(msg_)

    @property
    def file_path(self):
        """."""
        return self._filepath

    def get_perf(self, name: str):
        """."""
        for key, val in self._epochs.items():
            if val['name'] == name:
                return val
        return self._epochs.get(name, {})

    def summary(self, logger: Optional[Union[str, logging.Logger, Any]] = None):
        """."""

        pr_fn, _ = get_print_fcts(self._logger, logger)

        pr_fn('')
        pr_fn(str(self))
        pr_fn('')

        header_ = ['name', 'mapping', 'kind of operations', 'n_ops', 'n_cycles', 'r/w cycles', 'max cycles',
                   'n_ops/cycle C-M-R',
                   'r/w (Bytes)', 'BW (MB/s)', 'mpool:BW (MB/s) [r/w bytes]']
        rows_ = []
        for k, v in self._epochs.items():
            mem_accesses_ = v['mem_accesses']
            n_rw_ = 0
            n_rw_cycles_ = 0
            bw_per_mpool = ''
            for key, m_a_ in mem_accesses_.items():
                n_rw_ += (m_a_[0] + m_a_[2])
                n_rw_cycles_ += (m_a_[1] + m_a_[3])
                if m_a_[4]:
                    bw_per_mpool += f'{key}:{m_a_[4]:.3f} [{m_a_[0]}/{m_a_[2]}] '
            name_ = v['name']
            cycles_ratio_ = v["compute_cycles"] / v["max_cycles"] if v["max_cycles"] else 0.0
            bw_ = _compute_bw_mbs(n_rw_, v["max_cycles"])
            op_per_cycles_ = v["ops"] / v["compute_cycles"] if v["compute_cycles"] else 0.0
            m_op_per_cycles_ = v["ops"] / v["max_cycles"] if v["max_cycles"] else 0.0
            perc_m_cycles = v["max_cycles"] * 100 / self._n_max_cycles
            map_ = v["mapping"]
            fcts_ = ','.join([f'{v}x{k}' for k, v in v['functions'].items()]) if v['functions'] else '-'
            row_ = [name_,
                    map_,
                    fcts_,
                    f'{v["ops"]:,}',
                    f'{v["compute_cycles"]:,}',
                    f'{n_rw_cycles_:,}',
                    f'{v["max_cycles"]:,} {perc_m_cycles:6.2f}%',
                    f'{op_per_cycles_:5.1f}  {m_op_per_cycles_:5.1f}  {cycles_ratio_:4.2f}',
                    f'{n_rw_:,}',
                    f'{bw_:.3f}',
                    bw_per_mpool
                    ]
            rows_.append(row_)
        title_ = 'JSON info - mapping, memory accesses and cycles per epoch'
        colalign_ = ('left', 'left', 'left', 'right', 'right', 'right', 'right',
                     'right',
                     'right', 'right', 'left')
        print_table(header_, rows_, pr_fn, title=title_, colalign=colalign_)

    def __str__(self):
        """."""
        op_per_cycles_ = self._n_ops / self._n_compute_cycles
        m_op_per_cycles_ = self._n_ops / self._n_max_cycles
        msg_ = f'CNpuNetworkJson: {len(self._epochs)} n_ops={self._n_ops}'\
               f' cycles={self._n_compute_cycles} m_cycles={self._n_max_cycles}'\
               f' ops/cycle ideal/min={op_per_cycles_:.1f}/{m_op_per_cycles_:.1f}'
        return msg_
    


def parse_c_json_file(args):
    """Helper fct to parse a generated c-json file"""

    from datetime import datetime

    logger = logging.getLogger()

    logger.info('%s (version %s)', __title__, __version__)
    logger.info('Creating date : %s', datetime.now().ctime())
    logger.info('')
    logger.info('Input parameter : %s', args.input)

    try:
        json_r = CNpuNetworkJson(args.input, logger=logger, f_name=args.name)
        logger.info('Parsing the c-json file : %s', json_r.file_path)
        json_r.summary(logger)
        logger.info('')
    except ParserJsonError as ex:
        logger.error('Error: %s', ex)
        return 1
    
    return 0


def main():
    """Script entry point."""

    import argparse
    import os
    import sys

    sys.path.insert(1, os.path.join(os.path.dirname(__file__)))

    from logging_utilities import get_logger

    parser = argparse.ArgumentParser(
        description=f'{__title__} (v{__version__}) - {__author__}',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    DEFAULT_JSON_DIR = "st_ai_output/"

    parser.add_argument('--input', '-i', metavar='STR', type=str, help='generated json-file',
                        default=DEFAULT_JSON_DIR)
    
    parser.add_argument('--name', '-n', type=str, default='network',
                        help='Base name of the json file (e.g. network for network.json)')

    parser.add_argument('--log', metavar='STR', type=str, nargs='?',
                        default='no-log',
                        help='log file')

    parser.add_argument('--verbosity', '-v',
                        nargs='?', const=1, type=int, choices=range(0, 3),
                        default=1, help="set verbosity level")

    parser.add_argument('--debug', action='store_const', const=1,
                        help='Enable internal log (DEBUG PURPOSE)')

    parser.add_argument('--no-color', action='store_const', const=1,
                        help='Disable log color support')

    args = parser.parse_args()

    # configure logger
    lvl = logging.WARNING
    if args.verbosity > 0:
        lvl = logging.INFO
    if args.debug:
        lvl = logging.DEBUG

    if args.log is None:
        args.log = Path(__file__).stem + '.log'
    elif isinstance(args.log, str) and args.log == 'no-log':
        args.log = None

    logger = get_logger(level=lvl, color=not args.no_color, filename=args.log)

    try:
        res = parse_c_json_file(args)
    except ErrorException as e:
        logger.exception(e, stack_info=False, exc_info=args.debug)
        return -1

    return 0


if __name__ == '__main__':
    import sys
    sys.path.append(os.path.join(os.path.dirname(__file__), '.'))
    sys.exit(main())
